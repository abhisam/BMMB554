<!DOCTYPE html>
<html lang="en-us">
<head>
	<title>Duplex sequence analysis&middot; BMMB554 | Fall 2016</title>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="author" content="Anton Nekrutenko">
	<meta name="description" content="Introduction to data driven life sciences">
	<meta name="keywords" content="genomes, genetics, ngs, data analysis">
	<meta name="generator" content="Hugo 0.16" />

	<!-- CSS -->
	<link rel="stylesheet" href="http://nekrut.github.io/BMMB554/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="http://nekrut.github.io/BMMB554/favicon.ico" type="image/x-icon">

	<!-- RSS -->
	

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">

  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<body>
    <header class="site-header">
	<div class="branding">
		<a href="http://nekrut.github.io/BMMB554/">
		<img class="avatar" src="http://nekrut.github.io/BMMB554/img/gxy.png" alt=""/>
		</a>
		<h1 class="site-title">
			<a href="http://nekrut.github.io/BMMB554/">BMMB554 | Fall 2016</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
	      	<li><a href="/BMMB554/about/"> Syllabus </a></li>

	      	<li>
	<a href="http://nekrut.github.io/BMMB554/" title="">
		<i class="fa fa-fw fa-home"></i>
	</a>
</li>



<li>
	<a href="mailto:aun1@psu.edu" title="Email">
		<i class="fa fa-fw fa-envelope"></i>
	</a>
</li>











<li>
	<a href="https://github.com/nekrut" title="Github">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>



















<li>
	<a href="https://twitter.com/galaxyproject" title="Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>







	    </ul>
	</nav>
</header>


    <div class="content">
    <article class="feature-image">
		<header style="background-image: url('http://nekrut.github.io/BMMB554/img/topic10_cover.png')">
			<h1 class="title">Duplex sequence analysis</h1>
		</header>

	<section class="post-content">
		

<p>This page explains how to perform discovery of low frequency variants from duplex sequencing data. As an example we use the <em>ABL1</em> dataset published by <a href="http://www.nature.com/nmeth/journal/v12/n5/full/nmeth.3351.html">Schmitt and colleagues</a> (SRA accession <a href="http://www.ncbi.nlm.nih.gov/sra/?term=SRR1799908">SRR1799908</a>).</p>

<h1 id="background">Background</h1>

<p>Calling low frequency variants from next generation sequencing (NGS) data is challenging due to significant amount of noise characteristic of these technologies. <a href="http://www.pnas.org/content/109/36/14508.short">Duplex sequencing</a> (DS) was designed to address this problem by increasing sequencing accuracy by over four orders of magnitude. DS uses randomly generated barcodes to uniquely tag each molecule in a sample. The tagged fragments are then PCR amplified prior to the preparation of a sequencing library, creating fragment families characterized by unique combination of barcodes at both 5’ and 3’ ends:</p>

<blockquote>
<p><a href="http://www.pnas.org/content/109/36/14508/F1.expansion.html"><img src="/BMMB554/img/ds.png" alt="duplex" /></a></p>

<p>The logic of duplex sequencing. From <a href="http://www.pnas.org/content/109/36/14508.short">Schmitt:2012</a>.</p>
</blockquote>

<p>The computational analysis of DS data (Part <code>C</code> in the figure above) produces two kinds of output:</p>

<ul>
<li>Single Strand Consensus Sequences (SSCS; panel <code>iv</code> in the figure above);</li>
<li>Duplex Consensus Sequences (DCS; panel <code>v</code> in the figure above).</li>
</ul>

<p>The DCSs have the ultimate accuracy, yet the SSCSs can also be very useful when ampliconic DNA is used as an input to a DS experiment. Let us illustrate the utility of SSCSs with the following example. Suppose one is interested in quantifying variants in a virus that has a very low titer in body fluids. Since DS procedure requires a substantial amount of starting DNA (between <a href="http://nature.com/nprot/journal/v9/n11/full/nprot.2014.170.html">between 0.2 and 3 micrograms</a>) the virus needs to be enriched. This can be done, for example, with a PCR designed to amplify the entire genome of the virus. Yet the problem is that during the amplification heterologous strands will almost certainly realign to some extent forming hetoroduplex molecules:</p>

<blockquote>
<p><img src="/BMMB554/img/het.png" alt="hd" /></p>

<p>Heteroduplex formation in ampliconic templates. Image by Barbara Arbeithuber from <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1039-4">Stoler:2016</a>. Here there are two distinct types of viral genomes: carrying <code>A</code> and <code>G</code>. Because the population of genomes is enriched via PCR, heteroduplex formation takes place, skewing frequency estimates performed using DCSs.</p>
</blockquote>

<p>In the image above there are two alleles: green (A) and red (G). After PCR a fraction of molecules are in heteroduplex state. If this PCR-derived DNA is now used as the starting material for a DS experiment, the heteroduplex molecules will manifest themselves as having an <code>N</code> base at this site (because <em>Du Novo</em> interprets disagreements as <code>N</code>s during consensus generation). So, DSCs produced from this dataset will have <code>A</code>, <code>G</code>, and <code>N</code> at the polymorphic site. Yet, SSCSs will only have <code>A</code> and <code>G</code>. Thus SSCS will give a more accurate estimate of the allele frequency at this site in this particular case. In <em>Du Novo</em> SSCSs are generated when the <strong>Output single-strand consensus sequences</strong> option of <strong>Du Novo: Make consensus reads</strong> tool is set to <code>Yes</code> (see <a href="#generating-duplex-consensus-sequences-dcs">here</a>).</p>

<h2 id="how-to-use-this-tutorial">How to use this tutorial</h2>

<p>The entire analysis described here is accessible as a <a href="https://usegalaxy.org/u/aun1/h/duplex-analysis-abl1">Galaxy history</a> (by clicking on this link you can create your own copy and play with it).</p>

<blockquote>
<p><img src="http://galaxyproject.org/duplex/histItem.png" alt="History Item" /></p>

<p>Each history item has a Rerun <img src="http://galaxyproject.org/galaxy101/fa-refresh.png" alt="refresh" /> button. Clicking this button will show you how this tool was run with all parameters filled in exactly.</p>
</blockquote>

<p>This analysis (and consequently the Galaxy&rsquo;s history) can be divided into three parts
 1. Consensus generation from initial sequencing reads;
 2. Analysis of Duplex Consensus Sequences (DCS);
 3. Analysis of Single Strand Consensus Sequences (SSCS):</p>

<blockquote>
<p><img src="/BMMB554/img/steps.png" alt="steps" /></p>

<p>Analysis outline</p>
</blockquote>

<h1 id="start-generating-consensus-sequences">Start: Generating consensus sequences</h1>

<p>The starting point of the analyses are sequencing reads (usually in <a href="https://en.wikipedia.org/wiki/FASTQ_format">fastq</a> format) produced from a duplex sequencing library.</p>

<h2 id="getting-data-in-and-assessing-quality">Getting data in and assessing quality</h2>

<p>We uploaded <a href="http://www.nature.com/nmeth/journal/v12/n5/full/nmeth.3351.html">Schmitt:2015</a>) data directly from SRA as shown in <a href="https://vimeo.com/121187220">this screencast</a>. This created two datasets in our galaxy history: one for forward reads and one for reverse. We then evaluated the quality of the data by running FastQC on both datasets (forward and reverse) to obtain the following plots:</p>

<table>
<thead>
<tr>
<th align="left"></th>
<th align="left"></th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><img src="/BMMB554/img/abl1-f-qc.png" alt="" /></td>
<td align="left"><img src="/BMMB554/img/abl1-r-qc.png" alt="" /></td>
</tr>

<tr>
<td align="left"><strong>A</strong>. Forward</td>
<td align="left"><strong>B</strong>. Reverse</td>
</tr>
</tbody>
</table>

<p>One can see that these data are of excellent quality and no additional processing is required before we can start the actual analysis.</p>

<h2 id="generating-duplex-consensus-sequences-dcs">Generating Duplex Consensus Sequences (DCS)</h2>

<p>From tool section <strong>NGS: Du Novo</strong> we ran:</p>

<ol>
<li><strong>Make families</strong> (<code>Tag length = 12</code>; <code>Invariant sequence length = 5</code>)</li>
<li><strong>Align families</strong> (This is <strong>the most</strong> time consuming step of the workflow. It may take multiple days to run. The <em>ABL1</em> example took 34 hours and 7 minutes to finish. )</li>
<li><strong>Make consensus reads</strong> (<code>Minimum reads per family = 3</code>; <code>Minimum base quality = 20</code>; <code>FASTQ format = Sanger</code> ; <code>Output single-strand consensus sequences = Yes</code> :point_left: This is particularly important as explained below; also see the following image)</li>
</ol>

<p>This is the exact image of the <strong>Make consensus reads</strong> interface:</p>

<blockquote>
<p><img src="/BMMB554/img/makeCons.png" alt="Make consesni" /></p>

<p>Making DCS and SSCS. <strong>Note</strong> that <strong>Output single-strand consensus sequences</strong> is set to <code>Yes</code>. <a href="#background">Above</a> we explained why single-strand consensus sequences (SSCS) may be important in some applications. <a href="#analysis-of-single-strand-consensus-data">Below</a> we show how they can be used.</p>
</blockquote>

<h2 id="filtering-consensuses">Filtering consensuses</h2>

<p>The <em>Du Novo</em> algorithm occasionally inserts<code>N</code>and/or <a href="https://en.wikipedia.org/wiki/Nucleic_acid_notation">IUPAC notations</a> at sites where a definive base cannot be identified according to the major rule consensus. We however do not want such bases when we call variants. The tool <strong>Sequence Content Trimmer</strong> will help with filtering these out. Here are the parameters we used:</p>

<blockquote>
<p><img src="/BMMB554/img/contentTrimmer.png" alt="ContentTrimmer" /></p>

<p>Sequence Content Trimmer settings . Where:<br>- <code>Paired reads = Paired</code> (because DCSs are reported as forward and reverse)<br>- <code>Bases to filter on = NRYSWKMBDHV</code> (all ambiguous nucleotides)<br>- <code>Frequency threshold = 0.2</code> (A window /see the next parameter below/ cannot have more than 20% of ambiguous bases)<br>- <code>Size of the window = 10</code> (Size of the window)<br>- <code>Invert filter bases = No</code><br>- <code>Set a minimum read length = 50</code> (We do not want <em>very</em> short reads)</p>
</blockquote>

<h2 id="generating-fastq">Generating fastq</h2>

<p><a href="#filtering-consensuses">The previous step</a> filters forward and reverse DCSs and reports them in <a href="https://en.wikipedia.org/wiki/FASTA_format">FASTA</a> format. Yet the downstream tools require <a href="https://en.wikipedia.org/wiki/FASTQ_format">fastq</a> format. To address this we convert FASTA into fastq using <strong>Combine FASTA and QUAL</strong> from tool section <strong>NGS: QC and manipulation</strong>. In this case the quality values are filled in with the maximum allowed value of 93 (essentially we fake them here), which is fine as we will not rely on quality scores in the rest of the analysis.</p>

<blockquote>
<p><img src="/BMMB554/img/combineFandQ.png" alt="ContentTrimmer" /></p>

<p>Combine FASTA and QUAL. <strong>Note</strong> that here two datasets (#8 and #9) are selected simultaneously because we clicked the multiple datasets button the left of the <strong>FASTA File</strong> dropdown:<br> <img src="/BMMB554/img/multiDataset.png" alt="" /></p>
</blockquote>

<h2 id="calling-variants">Calling variants</h2>

<p>At this point we have trimmed DCSs in fastq format. We can now proceed to calling variants. This involves the following steps:</p>

<ol>
<li><a href="#align-against-genome-with-bwa-and-bwa-mem">Align against reference genome</a></li>
<li><a href="#merging">Merge results of multiple mappers</a> :point_left: This step is only useful if one uses multiple mappers (which we do here to show concordance. But this is not strictly necessary.)</li>
<li><a href="#left-aligning-indels">Left aligning indels</a></li>
<li><a href="#tabulating-the-differences">Tabulate the differences</a></li>
</ol>

<h3 id="align-against-genome-with-bwa-and-bwa-mem">Align against genome with <strong>BWA</strong> and <strong>BWA-MEM</strong></h3>

<p>Here we use two mappers for added reliability (this is not necessary in most situations as long as you use the right mapper for input data). To differentiate between results produced by each mapper we assign readgroups (this is done by clicking on <strong>Set read groups information</strong> dropdown). For example, for <strong>BWA-MEM</strong> you would set parameters like this:</p>

<blockquote>
<p><img src="/BMMB554/img/bwa-mem.png" alt="" /></p>

<p>Running BWA-MEM. <strong>Note</strong> that we are comparing DCSs against human genome version <code>hg38</code>, use forward and reverse DCSs are the <code>first</code> and <code>second</code> set of reads. Readgroup <strong>SM</strong> and <strong>ID</strong> tags are set <code>bwa-mem</code>.</p>
</blockquote>

<p>We then repeat essentially the same with <strong>BWA</strong>:</p>

<blockquote>
<p><img src="/BMMB554/img/bwa.png" alt="" /></p>

<p>Running BWA. <strong>Note</strong> here we use <code>bwa</code> as the readgroup <strong>ID</strong> and <strong>SM</strong> tags.</p>
</blockquote>

<h3 id="merging">Merging</h3>

<p>Since we have used two mappers - we have two BAM datasets. Yet because we have set readgroups we can now merge them into a single BAM dataset. This is because the individual reads will be labelled with readgroups (you will see how it will help later). To merge we use <strong>MergeSamFiles</strong> from tool section <strong>NGS: Picard</strong>:</p>

<blockquote>
<p><img src="/BMMB554/img/mergeSamFiles.png" alt="" /></p>

<p>Merging BAM datasets.</p>
</blockquote>

<h3 id="left-aligning-indels">Left Aligning indels</h3>

<p>To normalize the positional distribution of indels we use <strong>Left Align</strong> utility (<strong>NGS: Variant Analysis</strong>) from <a href="https://github.com/ekg/freebayes#indels">FreeBayes</a> package. This is necessary to avoid erroneous polymorphisms flanking regions with indels (e.g., in low complexity loci):</p>

<blockquote>
<p><img src="/BMMB554/img/leftAlign.png" alt="" /></p>

<p>Left aligning indels. <strong>Note</strong> here we use <code>hg38</code> as well. Obviously, one must use the same genome built you have aligned against with <strong>BWA-MEM</strong> and <strong>BWA</strong>.</p>
</blockquote>

<h3 id="tabulating-the-differences">Tabulating the differences</h3>

<p>To identify sites containing variants we use <strong>Naive Variant Caller (NVC)</strong> (tool section <strong>NGS: Variant Analysis</strong>) which produces a simple count of differences given coverage and base quality per site (remember that our qualities were &ldquo;faked&rdquo; during the conversion from FASTA to fastq and cannot be used here). So in the case of <em>ABL1</em> we set parameters as follow:</p>

<blockquote>
<p><img src="/BMMB554/img/nvc.png" alt="" /></p>

<p>Finding variants with NVC. Here:<br>- <code>Using reference genome = hg38</code> (As mentioned above, needs to be set to the same genome one have mapped against.)<br>- <code>Restrict to regions: Chromosome = chr9</code> (<em>ABL1</em> is on chromosome 9. We set this to prevent <strong>NVC</strong> from wandering across the genome to save time.)<br>- <code>Minimum number of reads needed to consider a REF/ALT = 0</code> (Trying to maximize the number of sites. We can filter later.)<br>- <code>Minimum base quality = 20</code> (This default and is irrelevant because of &ldquo;faking&rdquo; quality scores during the conversion from FASTA to fastq).<br>- <code>Minimum mapping quality = 20</code> (This is helpful because it prevents reads mapping to multiple locations from being included in the tabulation. Such reads will have mapping quality of 0.)<br>- <code>Ploidy = 1</code> (Ploidy is irrelevant here as it is a mixture of multiple genomes)<br>- <code>Only write out positions with possible alternate alleles = No</code> (We can filter later)<br>- <code>Report counts by strand = Yes</code> (This will be helpful to gauge the strand bias).</p>
</blockquote>

<p>The <strong>NVC</strong> generates a <a href="https://en.wikipedia.org/wiki/Variant_Call_Format">VCF</a> file that can be viewed at genome browsers such as <a href="https://www.broadinstitute.org/igv/">IGV</a>. Yet one rarely finds variants by looking at genome browsers. The next step is to generate a tab-delimited dataset of nucleotide counts using <strong>Variant Annotator</strong> from tool section <strong>NGS: Variant Analysis</strong>. We ran it with the following parameters:</p>

<blockquote>
<p><img src="/BMMB554/img/va.png" alt="" /></p>

<p>Annotating variable sites. Here <code>Coverage threshold = 10</code> (To reduce noise) and <code>Output stranded base counts = Yes</code> (to see strand bias)</p>
</blockquote>

<p>There are 3,264 lines in the output, which is clearly too much. Using <strong>Filter</strong> tool (tool section <strong>Filter and Sort</strong>) with expression <code>c16 &gt;= 0.01</code>(because column 16 contains minor allele frequency - MAF - and we are interested in those sites where MAF &gt;= 1%):</p>

<blockquote>
<p><img src="/BMMB554/img/filter.png" alt="" /></p>

<p>Filtering variable sites.</p>
</blockquote>

<p>will get that number to only 4 (showing just some of the columns):</p>

<table>
<thead>
<tr>
<th align="left">Mapper</th>
<th align="right">Position (chr9)</th>
<th align="center">Major allele</th>
<th align="center">Minor allele</th>
<th align="right">MAF</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">bwa</td>
<td align="right">130,872,141</td>
<td align="center">G</td>
<td align="center">A</td>
<td align="right">0.013</td>
</tr>

<tr>
<td align="left">bwa-mem</td>
<td align="right">130,872,141</td>
<td align="center">G</td>
<td align="center">A</td>
<td align="right">0.013</td>
</tr>

<tr>
<td align="left">bwa</td>
<td align="right">130,880,141</td>
<td align="center">A</td>
<td align="center">G</td>
<td align="right">0.479</td>
</tr>

<tr>
<td align="left">bwa-mem</td>
<td align="right">130,880,141</td>
<td align="center">A</td>
<td align="center">G</td>
<td align="right">0.479</td>
</tr>
</tbody>
</table>

<p>We can see that results of both mappers agree very well. The reason we see these numbers grouped by mappers is because we have set the readgroups while <a href="#align-against-genome-with-bwa-and-bwa-mem">mapping</a>.</p>

<p>The polymorphism we are interested in (and the one reported by <a href="http://www.nature.com/nmeth/journal/v12/n5/full/nmeth.3351.html">Schmitt:2015</a>) is at the position 130,872,141 and has a frequency of 1.3%. The other site (position 130,880,141) is a known common variant <a href="http://www.ncbi.nlm.nih.gov/SNP/snp_ref.cgi?type=rs&amp;rs=rs2227985">rs2227985</a>, which is heterozygous in this sample.</p>

<h1 id="analysis-of-single-strand-consensus-data">Analysis of single strand consensus data</h1>

<p>SSCSs are generated when the <strong>Output single-strand consensus sequences</strong> option of <strong>Du Novo: Make consensus reads</strong> tool is set to <code>Yes</code> (see <a href="#generating-duplex-consensus-sequences-dcs">here</a>). Analysis of SSCS data follows almost exactly the same trajectory. The only difference is that these <strong>do not</strong> come as forward and reverse. Instead <em>Du Novo</em> generates a single dataset. With this dataset we go through all the same steps:</p>

<ul>
<li><a href="#filtering-consensi">Filtering consensi</a></li>
<li><a href="#generating-fastq">Generating fastq</a></li>
<li><a href="#calling-variants">Calling variants</a>

<ul>
<li><a href="#align-against-genome-with-bwa-and-bwa-mem">Aligning against genome</a> (here the difference is that one needs to choose a single end option and use a single dataset as input)</li>
<li><a href="#merging">Merging</a></li>
<li><a href="#left-aligning-indels">Left aligning indels</a></li>
<li><a href="#tabulating-the-differences">Tabulating the differences</a></li>
</ul></li>
</ul>

<h2 id="repeating-this-analysis-using-workflows">Repeating this analysis using workflows</h2>

<p>The analysis described above can be rerun using a workflow. Workflow combined all steps into a single entity that only needs to be executed once. We provide two workflows:</p>

<ul>
<li><em>Du Novo</em> analysis from reads (import from <a href="https://usegalaxy.org/u/aun1/w/duplex-analysis-from-reads">here</a>). This workflow uses fastq reads as input. It should be used if you analyze data for first time.</li>
<li><em>Du Novo</em> analysis from aligned families (import from <a href="https://usegalaxy.org/u/aun1/w/copy-of-duplex-analysis-from-reads">here</a>). This workflow starts with aligned families. It should be used for re-analysis of already generated DCS and SSCS data.</li>
</ul>

<blockquote>
<p><a href="https://galaxyproject.org/duplex/fromReads.png"><img src="/BMMB554/img/fromReads.png" alt="" /></a></p>

<p>Starting from Reads</p>

<p><a href="https://galaxyproject.org/duplex/fromDCS.png"><img src="/BMMB554/img/fromDCS.png" alt="" /></a></p>

<p>Starting from DCS/SSCS data</p>
</blockquote>

<h2 id="if-things-don-t-work">If things don&rsquo;t work&hellip;</h2>

<p>&hellip;you need to complain. Use <a href="https://usegalaxy.org/biostar/biostar_redirect">Galaxy&rsquo;s BioStar Channel</a> to do this.</p>

	</section>
    </div>

    <footer class="site-footer">
	<p class="text">&copy; 2016 - Released under the MIT license<br>Powered by <a href="//gohugo.io/">Hugo</a> with the <a href="//github.com/digitalcraftsman/hugo-type-theme">Type Theme</a></p>
</footer>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>


<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-83364979-1', 'auto');
ga('send', 'pageview');
</script>

</body>
</html>
